# -*- coding: utf-8 -*-
"""CreditCardFraud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fvRPJK6R8mRFoLH2Y9XePptaNRvan80q
"""

import kagglehub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# Download latest version
path = kagglehub.dataset_download("mlg-ulb/creditcardfraud")

print("Path to dataset files:", path)

"""### EDA"""

data = pd.read_csv(f"{path}/creditcard.csv")
data.head()

print("Number of examples in the dataset " , data.shape[0])
print("Class imbalance in the dataset " , data['Class'].value_counts())

class_0_count = 284315
class_1_count = 492
total_samples = class_0_count + class_1_count

# Calculate class weights for imbalanced datasets
class_weights = {
    0: total_samples / (2 * class_0_count),
    1: total_samples / (2 * class_1_count)
}

print("Null values per column " , data.isnull().sum())

data = data.drop("Time",axis = 1)

models_to_test = ["LogisticRegression","XGBoost","NeuralNet"]

from sklearn.model_selection import train_test_split
y_train = data['Class']
X_train = data.drop(['Class'],axis = 1)
X_train , X_test, y_train , y_test = train_test_split(X_train ,y_train,test_size = 0.2,random_state = 42,shuffle = True)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train["Amount"] = scaler.fit_transform(X_train[["Amount"]])
X_test["Amount"] = scaler.transform(X_test[["Amount"]])

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score , average_precision_score
from xgboost import XGBClassifier

import tensorflow as tf

def simple_neural_net():
  inputs = tf.keras.layers.Input((29,))

  X = tf.keras.layers.Dense(200,'relu')(inputs)
  X = tf.keras.layers.BatchNormalization()(X)
  X = tf.keras.layers.Dense(200,'relu')(X)
  X = tf.keras.layers.BatchNormalization()(X)
  outputs = tf.keras.layers.Dense(1,'sigmoid')(X)

  model = tf.keras.Model(inputs = inputs , outputs = outputs)

  return model

models_to_test = ['LogisticRegression',"XGBoost","NeuralNet"]
model_results = {}
def model_trainer(model_name, X_train, y_train, X_test, y_test):
    if model_name == "LogisticRegression":
        model = LogisticRegression(max_iter=1000)  # Remove class_weight
        trained_model = model.fit(X_train, y_train)
        result = trained_model.predict_proba(X_test)[:, 1]
        score = average_precision_score(y_test, result)
        model_results[model_name] = score
    elif model_name == "XGBoost":
        model = XGBClassifier()  # Remove scale_pos_weight
        model = model.fit(X_train, y_train)
        result = model.predict_proba(X_test)[:, 1]
        score = average_precision_score(y_test, result)
        model_results[model_name] = score
    else:
        model = simple_neural_net()
        model.compile("adam", "BinaryCrossentropy",
                     metrics=[tf.keras.metrics.AUC(curve="PR", name='pr_auc')])
        history = model.fit(X_train, y=y_train, validation_data=[X_test, y_test],
                          epochs=2, verbose=1)  # Remove class_weight, increase epochs
        score = history.history['val_pr_auc'][-1]
        model_results[model_name] = score

def model_tester():
  for model in models_to_test:
    model_trainer(model,X_train_balanced,y_train_balanced,X_test,y_test)
  best_model = None
  max_score = 0
  for model_name,score in model_results.items():
    print(f"Model {model_name} average precision score = {score:.4f}")
    if score > max_score:
      max_score = score
      best_model = model_name

  print(f"The best model is {best_model} with an average precision score of {max_score:.4f}")
model_tester()

print(X_train_balanced.shape)
X_test.shape