# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ib8mExWpceY0aVZg_1ao4l-WtrCsNJfn
"""

import tensorflow as tf

model = tf.keras.applications.VGG19(
  include_top=False,
  input_shape=(240, 240, 3),  # FIXED: 400x400 like notebook
  weights='imagenet'
)
model.trainable = False

STYLE_LAYERS = [
    ('block1_conv1', 0.2),
    ('block2_conv1', 0.2),
    ('block3_conv1', 0.2),
    ('block4_conv1', 0.2),
    ('block5_conv1', 0.2)
]
content_layer = [('block5_conv4', 1)]

def get_layer_outputs(vgg, layer_names):
    outputs = [vgg.get_layer(layer[0]).output for layer in layer_names]
    model = tf.keras.Model([vgg.input], outputs)
    return model

vgg_model_outputs = get_layer_outputs(model, STYLE_LAYERS + content_layer)

# all_outputs = vgg_model_outputs(styleImg)
# a_s, a_s_1, a_s_2, a_s_3, a_s_4, a_c = all_outputs

# FIXED: Removed the poisonous /255.0 line completely
def gram_matrices_vectorized(a_a):
    shape_a = tf.shape(a_a) # Get symbolic shape tensor
    b , i , j , k = shape_a[0] , shape_a[1] , shape_a[2] , shape_a[3]
    grahm_matrix = tf.zeros([b,k,k])
    X = tf.reshape(a_a , (b,i*j,k))
    grahm_matrix= tf.matmul(X,X,transpose_a = True)
    return grahm_matrix

def style_cost(G_s, G_g, a_s):
    batch_size, n_h, n_w, n_c = a_s.shape[0], a_s.shape[1], a_s.shape[2], a_s.shape[3]

    sub = tf.reduce_sum(tf.square(tf.subtract(G_s, G_g)))
    norm = 4.0 * tf.cast(n_h * n_w, tf.float32) * tf.cast(n_c * n_c, tf.float32)

    result = tf.divide(sub, norm)
    return result

def content_cost(a_c, a_g_c):
    cost = 0.5 * tf.reduce_mean(tf.square(a_c - a_g_c))
    return cost

def total_variation_loss(img):
    x_deltas = img[:, :, 1:, :] - img[:, :, :-1, :]
    y_deltas = img[:, 1:, :, :] - img[:, :-1, :, :]
    return tf.reduce_mean(tf.abs(x_deltas)) + tf.reduce_mean(tf.abs(y_deltas))
def total_loss(a_c, a_s, a_g_s, a_g_c, alpha=0.1, beta=0.1):
    G_s = gram_matrices_vectorized(a_s)
    G_g = gram_matrices_vectorized(a_g_s)
    style = style_cost(G_s, G_g, a_s)
    content = content_cost(a_c, a_g_c)
    total_cost = alpha * content + beta * style
    return total_cost

from tensorflow.keras.layers import Input , Conv2D , MaxPooling2D , BatchNormalization , Conv2DTranspose
from tensorflow.keras.models import Model


def transformer_to_stylize():
  inputs = Input(shape= (240,240,3))

  # Encoder block
  # 398
  X = Conv2D(filters = 32,kernel_size = (3,3),padding = 'valid',activation = "relu")(inputs)
  X = BatchNormalization()(X)

  # 197
  X = Conv2D(filters = 64,kernel_size = (5,5),strides = (2,2),padding = 'valid',activation = "relu")(X)
  X = BatchNormalization()(X)

  # 98
  X = Conv2D(filters = 128,kernel_size = (3,3),strides = (2,2),padding = 'valid',activation = "relu")(X)
  X = BatchNormalization()(X)

  # Decoder Block

  # 197
  X = Conv2DTranspose(filters = 128,kernel_size = (3,3),strides = (2,2),padding = 'valid',activation = "relu")(X)
  X = BatchNormalization()(X)

  # 398
  X = Conv2DTranspose(filters = 64,kernel_size = (5,5),strides = (2,2),output_padding=(1,1),padding = 'valid',activation = "relu")(X) # FIXED: Changed output_padding to (1,1)
  X = BatchNormalization()(X)

  # 400
  X = Conv2DTranspose(filters = 32,kernel_size = (3,3),padding = 'valid',activation = "relu")(X) # No change needed, output_padding=0 is default
  X = BatchNormalization()(X)

  outputs = Conv2D(3,(3,3),activation = 'sigmoid', padding='same')(X) # FIXED: Added padding='same'
  model = Model(inputs = inputs , outputs = outputs)
  return model

import kagglehub

# Download latest version
path = kagglehub.dataset_download("awsaf49/coco-2017-dataset")

print("Path to dataset files:", path)

import os
dir = "/kaggle/input/coco-2017-dataset/coco2017/train2017"
filenames = os.listdir(dir)

tf_ds = tf.data.Dataset.from_tensor_slices(filenames[:1000])

def preprocess_ds(filename):
    filepath = tf.strings.join([dir, '/', filename])
    image = tf.io.read_file(filepath)
    decoded_image = tf.io.decode_jpeg(
        image,
        channels=3
    )
    resized_image = tf.image.resize_with_crop_or_pad(
        decoded_image,
        240,
        240
    )

    # Random horizontal flip
    augmented_image = tf.image.random_flip_left_right(
        resized_image
    )

    # Random saturation in a range, e.g. [0.5, 1.5]
    augmented_image = tf.image.random_saturation(
        augmented_image,
        lower=0.5,
        upper=1.5
    )

    augmented_image = tf.cast(augmented_image, tf.float32) / 255.0
    return augmented_image

import matplotlib.pyplot as plt
preprocessed_ds = tf_ds.map(preprocess_ds).batch(4).prefetch(tf.data.AUTOTUNE)
for x in preprocessed_ds.take(1):
  plt.imshow(x[1])

from tensorflow import keras
import tensorflow as tf
from tensorflow.keras.models import model_from_config # Explicit import

def load_preprocess_style():
    styleImg_bytes = tf.io.read_file("./style.jpg")
    styleImg = tf.io.decode_jpeg(styleImg_bytes, channels=3)
    styleImg = tf.image.resize_with_crop_or_pad(styleImg, 240, 240)
    # Add a batch dimension for the model input
    styleImg = tf.expand_dims(styleImg, axis=0)
    styleImg = tf.cast(styleImg, tf.float32) / 255.0
    return styleImg

@tf.keras.utils.register_keras_serializable()
class CustomModel(keras.Model):
    def __init__(self, transformer, vgg_loss_model, style_targets, alpha=8000, beta=0.1, tv_weight=3.0, **kwargs):
        super().__init__(**kwargs)
        self.transformer = transformer
        self.vgg = vgg_loss_model
        # Store original tensors for use in train_step
        self._style_targets = style_targets
        self.alpha = alpha
        self.beta = beta
        self.tv_weight = tv_weight

    # This property will expose style_targets as tensors for internal use
    @property
    def style_targets(self):
        return self._style_targets

    def call(self, inputs):
        return self.transformer(inputs)

    def train_step(self, data):
        x = data # Assuming data yields (input, target) pairs

        with tf.GradientTape() as tape:
            y_pred = self.transformer(x, training=True)
            content_outputs = self.vgg(x)
            gen_outputs = self.vgg(y_pred)

            *gen_style_feats, gen_content_feat = gen_outputs
            *content_style_dummy, content_content_feat = content_outputs

            style_loss_total = 0.0
            for target, gen in zip(self.style_targets, gen_style_feats):
                G_s = gram_matrices_vectorized(target)
                G_g = gram_matrices_vectorized(gen)
                style_loss_total += style_cost(G_s, G_g, target)
            style_loss_total /= len(self.style_targets)

            content_loss_val = content_cost(content_content_feat, gen_content_feat)
            tv_loss_val = total_variation_loss(y_pred)

            main_loss = self.alpha * content_loss_val + self.beta * style_loss_total
            loss = main_loss + self.tv_weight * tv_loss_val

        grads = tape.gradient(loss, self.transformer.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.transformer.trainable_variables))

        return {"loss": loss}

    def get_config(self):
        config = super().get_config()
        config.update({
            # Store the configuration dictionary of the sub-models
            "transformer_config": self.transformer.get_config(),
            "vgg_loss_model_config": self.vgg.get_config(),
            # Convert list of Tensors to a serializable list of lists/numpy arrays
            "style_targets_serializable": [t.numpy().tolist() for t in self._style_targets],
            "alpha": self.alpha,
            "beta": self.beta,
            "tv_weight": self.tv_weight,
        })
        return config

    @classmethod
    def from_config(cls, config):
        # Pop the custom parameters to pass to the custom __init__
        transformer_config = config.pop('transformer_config')
        vgg_loss_model_config = config.pop('vgg_loss_model_config')
        style_targets_serializable = config.pop('style_targets_serializable')
        alpha = config.pop('alpha')
        beta = config.pop('beta')
        tv_weight = config.pop('tv_weight')

        # Reconstruct the Keras models using the explicitly imported function
        transformer = model_from_config(transformer_config)
        vgg_loss_model = model_from_config(vgg_loss_model_config)

        # Reconstruct style_targets back to a list of Tensors
        style_targets = [tf.constant(t, dtype=tf.float32) for t in style_targets_serializable]

        # Instantiate the custom model using the reconstructed objects and other parameters
        return cls(transformer=transformer,
                   vgg_loss_model=vgg_loss_model,
                   style_targets=style_targets,
                   alpha=alpha,
                   beta=beta,
                   tv_weight=tv_weight,
                   **config)

style_img = load_preprocess_style()   # shape (1,400,400,3)
# vgg_model_outputs is the model that extracts features for loss calculation
style_targets = vgg_model_outputs(style_img)[:-1]      # all style layers (excluding the last one which is content)

# 3. Build transformer
transformer = transformer_to_stylize()

# 4. Wrap in CustomModel
# Pass vgg_model_outputs as the vgg_loss_model
nst_model = CustomModel(transformer, vgg_model_outputs, style_targets)
nst_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4))

# 5. Train
nst_model.fit(preprocessed_ds, epochs=1)

def load_preprocess():
    styleImg_bytes = tf.io.read_file("./DSC_7047.jpg")
    styleImg = tf.io.decode_jpeg(styleImg_bytes, channels=3)
    styleImg = tf.image.resize_with_crop_or_pad(styleImg, 240, 240)
    styleImg = tf.expand_dims(styleImg, axis=0)
    styleImg = tf.cast(styleImg, tf.float32) / 255.0
    return styleImg
image = load_preprocess()
result = nst_model.predict(image)
plt.imshow(tf.squeeze(result))
tf.keras.models.save_model(nst_model,"./test.keras")

pip install tf2onnx

import tf2onnx

model = tf.keras.models.load_model("./test.keras", custom_objects={'CustomModel': CustomModel})
input_signature = [tf.TensorSpec([None, 240, 240, 3], tf.float32, name="input")]
onnx_model, _ = tf2onnx.convert.from_keras(nst_model, input_signature, opset=13)
with open("model.onnx", "wb") as f:
    f.write(onnx_model.SerializeToString())

